# Awesome AI Benchmarks

A curated list of AI benchmarks across various domains including Natural Language Processing, Computer Vision, and Multimodal tasks. This repository automatically generates a searchable website at [awesome-ai-benchmarks.vercel.app](https://awesome-ai-benchmarks.vercel.app) from the benchmark entries below.

## üìä Statistics

- **Total Benchmarks**: 23
- **Categories**: 5
- **Subcategories**: 15
- **Last Updated**: 2025-08-01

## üöÄ Website

Visit our automatically generated website for a better browsing experience with search, filtering, and detailed benchmark information.

## üìù Contributing

We welcome contributions! Please read our [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to add new benchmarks or improve existing entries.

## üìö Benchmarks

### Programming & Code Generation

#### Code Generation & Evaluation
- **LiveCodeBench** - A comprehensive benchmark for evaluating code generation capabilities of large language models on real-world programming tasks
  - Paper: https://arxiv.org/abs/2403.07974
  - Website: https://livecodebench.github.io/
  - Year: 2024

- **can-ai-code** - Simple evaluation framework for code generation models with multiple programming languages
  - Code: https://github.com/the-crypt-keeper/can-ai-code
  - Website: https://huggingface.co/spaces/mike-ravkine/can-ai-code-results
  - Year: 2024

- **Kotlin-bench** - Benchmark specifically designed for evaluating Kotlin programming capabilities
  - Website: https://firebender.com/blog/kotlin-bench
  - Year: 2024

#### API & Tool Usage
- **Gorilla** - Large Language Model Connected with Massive APIs for function calling evaluation
  - Website: https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html#bfcl
  - Year: 2023

- **ToolSandbox** - A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities
  - Paper: https://arxiv.org/abs/2408.04682
  - Year: 2024

#### Terminal & Environment
- **terminal-bench** - A benchmark for AI agents in terminal environments
  - Website: https://www.tbench.ai
  - Year: 2024

#### Logic & Reasoning
- **Sudoku-Bench** - Benchmark testing logical reasoning capabilities through Sudoku puzzle solving
  - Code: https://github.com/SakanaAI/Sudoku-Bench
  - Website: https://pub.sakana.ai/sudoku/
  - Year: 2024

### Multimodal & Vision

#### Video Understanding
- **LongVideoBench** - A Benchmark for Long-context Interleaved Video-Language Understanding
  - Paper: https://arxiv.org/abs/2407.15754
  - Website: https://longvideobench.github.io/
  - Year: 2024

#### Multimodal Evaluation
- **MLLM-as-a-Judge** - Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
  - Paper: https://arxiv.org/abs/2402.04788
  - Website: https://mllm-judge.github.io/
  - Year: 2024

- **Vision Language Models are Biased** - Benchmark examining bias in vision-language models
  - Website: https://vlmsarebiased.github.io/
  - Year: 2024

#### OCR & Document Understanding
- **olmOCR-Bench** - Benchmark for evaluating optical character recognition capabilities
  - Paper: https://arxiv.org/abs/2502.18443
  - Code: https://github.com/allenai/olmocr/tree/main/olmocr/bench
  - Year: 2025

### Legal & Domain-Specific

#### Legal Reasoning
- **LEXam** - Benchmarking Legal Reasoning on 340 Law Exams from Swiss, EU, and international law examinations
  - Paper: https://arxiv.org/abs/2505.12864
  - Code: https://github.com/LEXam-Benchmark/LEXam/tree/main
  - Year: 2024

- **CaseLaw** - Legal case law analysis and reasoning benchmark
  - Website: https://www.vals.ai/benchmarks/case_law-05-09-2025
  - Year: 2025

### Agent Capabilities & Reasoning

#### Long-term Coherence
- **Vending-Bench** - A Benchmark for Long-Term Coherence of Autonomous Agents
  - Paper: https://arxiv.org/pdf/2502.15840
  - Website: https://andonlabs.com/evals/vending-bench
  - Year: 2025

#### Scientific & Academic Reasoning
- **OlympiadBench** - A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
  - Paper: https://arxiv.org/pdf/2402.14008
  - Code: https://github.com/OpenBMB/OlympiadBench
  - Year: 2024

#### Security & Robustness
- **JailbreakBench** - An Open Robustness Benchmark for Jailbreaking Large Language Models
  - Paper: https://arxiv.org/abs/2404.01318
  - Website: https://jailbreakbench.github.io
  - Year: 2024

- **SnitchBench** - Benchmark for evaluating model safety and information leakage
  - Website: https://snitchbench.t3.gg/
  - Code: https://github.com/t3dotgg/SnitchBench
  - Year: 2024

#### Business & CRM
- **CRMArena-Pro** - Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions
  - Paper: https://arxiv.org/abs/2505.18878
  - Website: https://www.salesforce.com/blog/crmarena-pro/
  - Year: 2025

### Creative & Evaluation

#### Memory & Episodic Tasks
- **Episodic Memories Generation and Evaluation Benchmark** - Benchmark for Large Language Models memory capabilities
  - Paper: https://arxiv.org/abs/2501.13121
  - Year: 2025

#### Creative Writing
- **Longform Creative Writing** - Benchmark for evaluating long-form creative writing capabilities
  - Website: https://eqbench.com/creative_writing_longform.html
  - Year: 2024

- **Creative Writing v3** - Enhanced creative writing evaluation benchmark
  - Website: https://eqbench.com/creative_writing.html
  - Year: 2024

#### Judgment & Analysis
- **Judgemark v2** - Benchmark for evaluating judgment and decision-making capabilities
  - Website: https://eqbench.com/judgemark-v2.html
  - Year: 2024

- **BuzzBench** - Humor analysis benchmark for evaluating understanding of comedy and wit
  - Website: https://eqbench.com/buzzbench.html
  - Year: 2024

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Acknowledgments

- Thanks to all the researchers and organizations who created these benchmarks
- Inspired by other "awesome" lists in the open source community
